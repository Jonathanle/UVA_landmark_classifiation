{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4u4YC8iqxpe"
      },
      "source": [
        "# CODEATHON 2: Recognizing UVA landmarks with neural nets (50 pts)\n",
        "![UVA Grounds](https://giving.virginia.edu/sites/default/files/2019-02/jgi-teaser-image.jpg)\n",
        "\n",
        "The UVA Grounds is known for its Jeffersonian architecture and place in U.S. history as a model for college and university campuses throughout the country. Throughout its history, the University of Virginia has won praises for its unique Jeffersonian architecture.\n",
        "\n",
        "In this codeathon, you will attempt the build an image recognition system to classify different buildlings/landmarks on Grounds. You will earn 50 points for this codeathon plus 10 bonus points. To make it easier for you, some codes have been provided to help you process the data, you may modify it to fit your needs.\n",
        "\n",
        "You must submit the .ipynb file via UVA Collab with the following format: **yourUVAComputingID_codeathon_2.ipynb**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIAunzfaCNAH",
        "outputId": "5fe6566a-be65-405b-d6c0-f02660bbf240"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Note to self: I will learn how to vallidate machien learning workflows with tthe simple model first approach --> helps with baseline\n",
        "\n",
        "- Get a specific focus on whether I know and verify mentally key representaations\n",
        "- can i use tools cororectly in order to build in tension to me wanting to build as fast as possible?\n",
        "- think about specific long term dependencies that might happen especiaalyl with coding and make practices now that will prevent the error - esp with learning \n",
        "0 jupyter note book + library hybrid for better communication.\n",
        "\n",
        "- see how rigorousness contradicts with the perceived amount of time -> I decide to be too rigorous, does this actually reduce the amount of time i have from later debuggin?\n",
        "\n",
        "- See methods for debugging and verification of the dataset + form attributes\n",
        "\n",
        "\n",
        "\n",
        "- Representational issue - Representing Batch Sizes + Relation in TF to othe training --> Why is f1 score going wronig? \n",
        "- how do we know that the fit functions for CNN are the same with the data? --> data alignment problem.\n",
        "- Rep 1 - What is the dataset tf outputting? --> categorical labels or one hot encodings\n",
        "- in representation of code - i dont know \"hwat is happenign\" nor can i anticipate the representations. \n",
        "---> here somehow in achieving a better representation I can heuristically analyze by removing complexity and seeing if any key interactions take place.\n",
        "\n",
        "\n",
        "- i really want to add the recall metrics to tensorflow -- there is some sort of internal error with the interface - some sort of interaction between the dataset model, and recall fucntion is happening.\n",
        "    - problem 1 - tf dataset is hard to represent rather than using explicit tensors.\n",
        "\n",
        "# Relataoin to other types of learnign tasks \n",
        "- here it is much easier to slow down and step back as I can always expand my time to fit the cogntive  load, there are no time dependencies \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "# Problems I solved\n",
        "\n",
        "1. I establish a baseline model - \"this is the refeerence point i have bad performance\" - > i want to know if changing model parameters actually does welll and incrases performacne --> identifies changes in model to outcomes\"\n",
        "    here convergence also informs aabout hyperparametres that need to change in context to the goal of minimized time. - bang for your buck convergence.\n",
        "3. How do I monitor how well the model is learning and converging? --> how do i interpert plateauing performance?\n",
        "4. Can i implement flows that dyanmically adjust epoochs for convergence? like when does it stop?\n",
        "2. Metrics - How can I fit the code so taht it can explain to me how well hte model is doing?\n",
        "    - idea w precisoin + recall - we need probabilities NOT logits, this had to do with a model output problem.\n",
        "\n",
        "4. sB1 - How doo i find errors in a mach8ien learning pipeline? --> I want to simplify the dataset and attributes and observe interface wise what is happening - devise ways of understanding\n",
        "    - use git to \"expierment\" instead of creating feature --> time spent - 1hr of learning \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import sys\n",
        "import sklearn\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "\n",
        "#%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from logging_practice import setup_logger, log_function\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whpECRG_B2Nj"
      },
      "source": [
        "# Step 1: Process the  Dataset\n",
        "The full dataset is huge (+37GB) with +13K images of 18 classes. So it will take a while to download, extract, and process. To save you time and effort, a subset of the data has been resized and compressed to only 379Mb and stored in my Firebase server. This dataset will be the one you will benchmark for your grade. If you are up for a challenge (and perhaps bonus points), contact the instructor for the full dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4E5kiDN9OFs",
        "outputId": "e50ba47c-0b57-493e-ce9b-291d3e253b73"
      },
      "outputs": [],
      "source": [
        "# Download dataset from Firebase\n",
        "#!wget https://firebasestorage.googleapis.com/v0/b/uva-landmark-images.appspot.com/o/dataset.zip?alt=media&token=e1403951-30d6-42b8-ba4e-394af1a2ddb7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKWszcAd9WJh",
        "outputId": "f2be69bf-3c0a-4c80-ec57-1d8092b0d9f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "New-Object : Exception calling \".ctor\" with \"3\" argument(s): \"End of Central Directory record could not be found.\"\n",
            "At \n",
            "C:\\WINDOWS\\system32\\WindowsPowerShell\\v1.0\\Modules\\Microsoft.PowerShell.Archive\\Microsoft.PowerShell.Archive.psm1:934 \n",
            "char:23\n",
            "+ ... ipArchive = New-Object -TypeName System.IO.Compression.ZipArchive -Ar ...\n",
            "+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    + CategoryInfo          : InvalidOperation: (:) [New-Object], MethodInvocationException\n",
            "    + FullyQualifiedErrorId : ConstructorInvokedThrowException,Microsoft.PowerShell.Commands.NewObjectCommand\n",
            " \n"
          ]
        }
      ],
      "source": [
        "# Extract content\n",
        "#TODO: Change this back so that powershell version is desired\n",
        "#!unzip \"/content/dataset.zip?alt=media\"\n",
        "\n",
        "!powershell Expand-Archive -Path dataset.zip -DestinationPath ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "474nVh3m-FrM",
        "outputId": "e3244489-6684-43d4-af07-7f8108715fad"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'int' object has no attribute 'info'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8596\\2932626371.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;31m# Training Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m \u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dummy_tf_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\jonathanle\\CS\\UVA landmark Classification\\logging_practice.py\u001b[0m in \u001b[0;36mnew_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# emphasizes during statidc runtime the function definition helps to check it well --> typecheckign will happen runtime anyway, but i want the same behavior.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[1;32mdef\u001b[0m \u001b[0mnew_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"calling new_function outside level parmeter: {func.__name__}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'info'"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_files\n",
        "#from keras.utils import np_utils\n",
        "\n",
        "#from keras.preprocessing import imag\n",
        "\n",
        "from tqdm import tqdm # progress bar\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" \n",
        "TODO: \n",
        "\n",
        "Learn more rigoroulsy about the code why certain processes happen / debugs\n",
        "---> Learna exactly the behavior of my model --> use those insights to - i want the deiling here to be as high as possibel for model interpretation.\n",
        "Simplify + Log the interactions\n",
        "- provide methods if simplification \n",
        "- provide methods of rigorous logging - better than print statements and I can turn off and on.\n",
        "- other contexts - printing the losses, visualization of results.\\\\\n",
        "\n",
        "- later on use / review the code clutttering while - interfacing andd logging other actions - consider ideas of decorators\n",
        "-\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import logging\n",
        "\n",
        "logger = setup_logger(level = logging.INFO)\n",
        "\n",
        "\n",
        "\n",
        "@log_function(logger, logging.INFO)\n",
        "def create_dummy_tf_datasets(hyperparameters):\n",
        "    # Set seeds for reproducibility\n",
        "    tf.random.set_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "\n",
        "    batch_size, img_height, img_width, n_channels, n_classes, n_total_images = hyperparameters\n",
        "\n",
        "\n",
        "    \n",
        "    # Create dummy images and labels\n",
        "    # Using random values between 0 and 255 for images\n",
        "    dummy_images = tf.random.uniform(\n",
        "        shape=(n_total_images, img_height, img_width, n_channels),\n",
        "        minval=0,\n",
        "        maxval=255,\n",
        "        dtype=tf.float32\n",
        "    )\n",
        "    \n",
        "    # Create dummy labels (alternating between 0 and 1)\n",
        "    dummy_labels = tf.constant([i % n_classes for i in range(n_total_images)])\n",
        "    \n",
        "    # Split into training and validation\n",
        "    # Training: 8 images (80%), Validation: 2 images (20%)\n",
        "    train_size = int(0.8 * n_total_images)\n",
        "    \n",
        "    # Split the data\n",
        "    train_images = dummy_images[:train_size]\n",
        "    train_labels = dummy_labels[:train_size]\n",
        "    val_images = dummy_images[train_size:]\n",
        "    val_labels = dummy_labels[train_size:]\n",
        "    \n",
        "    # Create datasets\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "    validation_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "    \n",
        "    # Configure datasets to match tf.keras.preprocessing.image_dataset_from_directory behavior\n",
        "    # Batch, shuffle, and repeat\n",
        "    train_ds = train_ds.shuffle(buffer_size=1000, seed=42)\n",
        "    train_ds = train_ds.batch(batch_size)\n",
        "    train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    validation_ds = validation_ds.batch(batch_size)\n",
        "    validation_ds = validation_ds.prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    # Add class_names attribute to match directory-loaded dataset behavior\n",
        "    class_names = ['class1', 'class2']\n",
        "    train_ds.class_names = class_names\n",
        "    validation_ds.class_names = class_names\n",
        "    \n",
        "    return train_ds, validation_ds\n",
        "\n",
        "\n",
        "# TODO: Change this back to content/dataset\n",
        "data_dir = \"./dataset/\"\n",
        "\n",
        "\n",
        "batch_size = 2\n",
        "img_height = 30\n",
        "img_width = 30\n",
        "n_channels = 2\n",
        "num_classes = 2\n",
        "n_total_images = 10  # Total number of images\n",
        "\n",
        "\n",
        "hyperparameters = [batch_size, img_height, img_width, n_channels, num_classes, n_total_images]\n",
        "\n",
        "\n",
        "# Dataset that loads the specific portion using \"subset\" parameter\n",
        "\n",
        "\n",
        "# Training Dataset\n",
        "train_ds, validation_ds = create_dummy_tf_datasets(hyperparameters)\n",
        "\n",
        "\n",
        "# Convert the labels into one hot encodings \n",
        "\n",
        "#train_ds = train_ds.map(lambda x, y: (x, tf.one_hot(y, depth=18)))\n",
        "#validation_ds = validation_ds.map(lambda x, y: (x, tf.one_hot(y, depth=18)))\n",
        "\n",
        "\n",
        "print(train_ds)\n",
        "print(validation_ds)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x_HSPOCk4uP",
        "outputId": "1c06652b-96ea-4907-d0f8-2b09ad830a3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.prefetch_op._PrefetchDataset"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5gWDUhbDDFZn",
        "outputId": "ef8b46b9-c35d-4160-d647-57600843db2c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP0-jzrMYioK"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE STARTS HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJmFciPVn3YY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L5V5__CnzWP"
      },
      "source": [
        "# Step 2: Create your own CNN architecture\n",
        "You must design your own architecture. To get started, you may get inspiration from one in CNN notebook  (i.e. use one similar to LeNet-5 or AlexNet). You will have to report the design of the architecture:\n",
        "\n",
        "1.   How many layers does it have?\n",
        "2.   Why do you decide on a certain number nodes per layer?\n",
        "3.   Which activation functions do you choose?\n",
        "4.   How many parameters does it has in total?\n",
        "\n",
        "Hint: use `myModel.summary()` to learn on the layers and parameters\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZfEgQIUnzWY"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_20\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_20\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_75 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m3\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_75 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_76 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m2\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_76 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_77 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m2\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_77 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_21 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m100\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">107</span> (428.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m107\u001b[0m (428.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">107</span> (428.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m107\u001b[0m (428.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'\\nI have created a CNN network that contains 3 layers of convolutions and pooling foolowed by a Dense Linear layer, that outputs logits for a class. For the number of nodes per layer, I indirectly decidedd the number of nodes per layer\\nthrouogh teh kernel size of (3,3) because I thought that heuristically that this was a reasonable starting point for helping the model process local features without being overwhelmed by a large kernel size. I decided to use a RELU function\\nbecause this was computationally easy to compute. ReLU fucntionos \\n\\nMy model has a total of 1,605,458 parameters\\n\\n\\n'"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your code here\n",
        "# Design a Model that does really welll in classifying images\n",
        "# Clearly a CNN architectures is useful here because we dont reallly need to process long term dependencies in order to build the building featuresa\n",
        "# Use a baseline model + hyperparameters to make a baseline evaluation --> establishes a reference point for showing if it is performinb well.\n",
        "\n",
        "\n",
        "\n",
        "myModelBaseLine = keras.models.Sequential([\n",
        "    # First conv-pool block\n",
        "    keras.layers.Conv2D(1, (1, 1), activation='relu', input_shape=(img_height, img_width, n_channels)), # idea 1 - input shape must reflect batch sizes\n",
        "    keras.layers.MaxPooling2D(1, 1),\n",
        "    \n",
        "    # Second conv-pool block\n",
        "    keras.layers.Conv2D(1, (1, 1), activation='relu'),\n",
        "    keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    # Third conv-pool block\n",
        "    keras.layers.Conv2D(1, (1,1), activation='relu'),\n",
        "    keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    # Flatten and dense layer\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(num_classes, activation = 'softmax'), # No activation for logits\n",
        "\n",
        "])\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Third conv-pool block\n",
        "    keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D(2,2),\n",
        "# Third conv-pool block\n",
        "keras.layers.Conv2D(512, (3,3), activation='relu'),\n",
        "keras.layers.MaxPooling2D(2,2),\n",
        "\"\"\" \n",
        "myModelBaseLine.summary()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "I have created a CNN network that contains 3 layers of convolutions and pooling foolowed by a Dense Linear layer, that outputs logits for a class. For the number of nodes per layer, I indirectly decidedd the number of nodes per layer\n",
        "throuogh teh kernel size of (3,3) because I thought that heuristically that this was a reasonable starting point for helping the model process local features without being overwhelmed by a large kernel size. I decided to use a RELU function\n",
        "because this was computationally easy to compute. ReLU fucntionos \n",
        "\n",
        "My model has a total of 1,605,458 parameters\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33s9AMe6ok9x"
      },
      "source": [
        "After designing the model, you will need to train it. In order to train, you will need to pick a number of `epoch` (iteration), which `optimizer` to use (from  `keras.optimizers`), a `loss` function, and some `metrics`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYsKzVh0pKRb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node LogicalAnd_1 defined at (most recent call last):\n  File \"c:\\tools\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n\n  File \"c:\\tools\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n\n  File \"c:\\tools\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n\n  File \"c:\\tools\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n\n  File \"c:\\tools\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n\n  File \"C:\\Users\\jonathanle\\AppData\\Local\\Temp\\ipykernel_8596\\3526912466.py\", line 20, in <module>\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 314, in fit\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 117, in one_step_on_iterator\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 104, in one_step_on_data\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 73, in train_step\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 412, in compute_metrics\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 330, in update_state\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 17, in update_state\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\metrics\\confusion_metrics.py\", line 378, in update_state\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\metrics\\metrics_utils.py\", line 592, in update_confusion_matrix_variables\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\metrics\\metrics_utils.py\", line 565, in weighted_assign_add\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\ops\\numpy.py\", line 3242, in logical_and\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 1409, in logical_and\n\nIncompatible shapes: [1,4] vs. [1,2]\n\t [[{{node LogicalAnd_1}}]] [Op:__inference_one_step_on_iterator_161173]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8596\\3526912466.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mmyModelBaseLine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmyLoss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyOptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyMetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m history = myModelBaseLine.fit(train_ds,\n\u001b[0m\u001b[0;32m     21\u001b[0m                       \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                       epochs = myEpochs)\n",
            "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node LogicalAnd_1 defined at (most recent call last):\n  File \"c:\\tools\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n\n  File \"c:\\tools\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n\n  File \"c:\\tools\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n\n  File \"c:\\tools\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n\n  File \"c:\\tools\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n\n  File \"C:\\Users\\jonathanle\\AppData\\Local\\Temp\\ipykernel_8596\\3526912466.py\", line 20, in <module>\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 314, in fit\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 117, in one_step_on_iterator\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 104, in one_step_on_data\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 73, in train_step\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 412, in compute_metrics\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 330, in update_state\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 17, in update_state\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\metrics\\confusion_metrics.py\", line 378, in update_state\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\metrics\\metrics_utils.py\", line 592, in update_confusion_matrix_variables\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\metrics\\metrics_utils.py\", line 565, in weighted_assign_add\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\ops\\numpy.py\", line 3242, in logical_and\n\n  File \"c:\\tools\\Anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 1409, in logical_and\n\nIncompatible shapes: [1,4] vs. [1,2]\n\t [[{{node LogicalAnd_1}}]] [Op:__inference_one_step_on_iterator_161173]"
          ]
        }
      ],
      "source": [
        "myEpochs = 8 # can i craet a dynamic convergence of this?\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "# Complexities --> is this right \"thing\" that is used to optimimze?\n",
        "myOptimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=learning_rate,\n",
        "    name='Adam',\n",
        ")\n",
        "myLoss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False)\n",
        "myMetrics = myMetrics = [\n",
        "    'accuracy',  \n",
        "    tf.keras.metrics.Precision(),                                                                                                                    # Standard accuracy\n",
        "]\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "myModelBaseLine.compile(loss= myLoss, optimizer = myOptimizer, metrics = myMetrics)\n",
        "history = myModelBaseLine.fit(train_ds,\n",
        "                      validation_data = validation_ds,\n",
        "                      epochs = myEpochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDLqsjlkoNay"
      },
      "source": [
        "Next, you need to create (1) a plot of training and validation `loss` and (2) a plot of training and validation `accuracy`. These plots might give you some insights about your model performance and possibility of overfitting.\n",
        "\n",
        "Report the performance of your architecture on the validation set in a `confusion matrix`. Make comments on the performance by answering the following questiosns:\n",
        "- How well do you think your architecture is doing (overall accuracy)?\n",
        "- Where did it makes mistake most?\n",
        "- Which classes can be improved?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FhpZM_ErHqn"
      },
      "outputs": [],
      "source": [
        "# Your evaluation code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJF5CTcOl2Fy"
      },
      "source": [
        "# Step 3: Use a Pre-trained Network with Transfer Learning\n",
        "Now that you have a your own custom model and some baseline performance, let's see if you can improve the performance using transfer learning and a pre-trained model. You may use any pre-trained model EXCEPT ones that already provided such as `Xception`, `MobileNet`, `EfficientNetB6`. Keep in mind that each pre-trained model may expect a different input shape, so adjust the size of your training images accordingly.\n",
        "\n",
        "Make sure you report the design of this architecture by answer the same questions 1-4 in Step 3.\n",
        "\n",
        "Hint: use `ImageNet` as weights when load the pre-train network, then add a `GlobalAveragePooling2D` and an output layer with `softmax` activation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJlRl1i-l970"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_13nh6H9uXWm"
      },
      "source": [
        "Next, you will attempt to adapt this pre-trained model to your UVA Landmark dataset. It is recommended that you tried the two-phase training approach for your model:\n",
        "\n",
        "1.   Phase 1: Freeze the pre-train weights and only train the top layer\n",
        "2.   Phase 2: Train the entire network with much smaller learning rate (adapt the model to UVA data, but avoid destroying the transfered weights).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pChHvQqVvfPL"
      },
      "outputs": [],
      "source": [
        "# Phase 1 code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swnOoePTvhyH"
      },
      "outputs": [],
      "source": [
        "# Phase 2 code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0qwDhE9uHW2"
      },
      "source": [
        "Repeat the same reporting of performance using the confusion matrix:\n",
        "- Did this pre-trained network do better overall?\n",
        "- In which class it improve the accuracy from the above model?\n",
        "- Which class still has low performance?\n",
        "\n",
        "Typically, your network must have a reasonable performance of at least 84% overall accuracy to be considered successful in this domain. If your network achieves a accuracy of 94% or above on the validation set, you will also recieve a 10 bonus points, so keep trying!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPmNtIGVl-7F"
      },
      "source": [
        "# Step 4: Reflection\n",
        "\n",
        "Write at least a paragraph answering these prompts: How did your own network perform in comparison to the pre-trained one? What are the major differences between the architectures? Additionally, report on your experience implementing different models for this assignment (Was it hard/easy/fun?, from which part did you learn the most?)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YtdjSICw66_"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
